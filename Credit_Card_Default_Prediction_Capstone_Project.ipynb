{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashraj2022/credit-card-prediction/blob/main/Credit_Card_Default_Prediction_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Project Name --: **Credit_Card_Default_Prediction_Capstone_Project**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Type - **Classification**\n",
        "\n",
        "Contribution - **Individual**/\n",
        "\n",
        "Name : **Akash** **Raj**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments.\n",
        "\n",
        "In recent years, the credit card issuers in Taiwan faced the cash and credit card debt crisis and the delinquency is expected to peak in the third quarter of 2006 (Chou,2006). In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, overused credit card for consumption and accumulated heavy credit and cash–card debts. The crisis caused the blow to consumer finance confidence and it is a big challenge for both banks and cardholders\n",
        "\n",
        "The above dataset has 30000 rows and 25 columns. There are no mising values and duplicate values in the dataset.\n",
        "\n",
        "In today’s world credit cards have become a lifeline to a lot of people so banks provide us with credit cards. Now we know the most common issue there is in providing these kind of deals are people not being able to pay the bills. These people are what we call “defaulters”.Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments.**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        " - Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        " - Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        " - Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "- Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, plot_precision_recall_curve\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data =\"https://raw.githubusercontent.com/akashraj2022/credit-card-prediction/main/default%20of%20credit%20card%20clients.xls?raw=true\"\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_excel(data)"
      ],
      "metadata": {
        "id": "OGKJqWybmwJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading data\n",
        "df=pd.read_excel(data,header=1)\n",
        "df"
      ],
      "metadata": {
        "id": "Dzsi8orWm4dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last 5 \n",
        "df.tail()"
      ],
      "metadata": {
        "id": "YY9l_je0nKHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulizing                          #true means duplicate data\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.countplot(x=df.duplicated())"
      ],
      "metadata": {
        "id": "DBU38USMol4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        " # Checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"Missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In recent years, the credit card issuers in Taiwan faced the cash and credit card debt crisis and the delinquency is expected to peak in the third quarter of 2006 (Chou,2006). In order to increase market share, card-issuing banks in Taiwan over-issued cash and credit cards to unqualified applicants. At the same time, most cardholders, irrespective of their repayment ability, overused credit card for consumption and accumulated heavy credit and cash–card debts. The crisis caused the blow to consumer finance confidence and it is a big challenge for both banks and cardholders**\n",
        "\n",
        "\n",
        "**The above dataset has 30000 rows and 25 columns. There are no mising values and duplicate values in the dataset.**\n",
        "\n",
        "# Data Description\n",
        "# Attribute Information:\n",
        "\n",
        "**This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:**\n",
        "\n",
        "X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "\n",
        "X2: Gender (1 = male; 2 = female).\n",
        "\n",
        "X3: Education(1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "\n",
        "X4: Marital status (1 = married; 2 = single; 3 = others).\n",
        "\n",
        "X5: Age (year).\n",
        "\n",
        "X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "\n",
        "X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
        "X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 30,000 distinct credit card clients.\n",
        "\n",
        "The average value for the amount of credit card limit is 167484. The standard deviation is unusually large, max value being 1000000.\n",
        "\n",
        "Education Level is mostly graduate school and university.\n",
        "\n",
        "Most of the clients are either married or single .\n",
        "\n",
        "Average age is 35.5 years, with a standard deviation of 9.2\n",
        "\n",
        "As the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis). "
      ],
      "metadata": {
        "id": "FSKgLmRwuJee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Dataset and its attributes are described below**\n",
        "\n",
        " 1. ID: ID of each client\n",
        " 2. LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "3. SEX: Gender (1=male, 2=female)\n",
        "4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
        "6. AGE: Age in years\n",
        "7. PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months,8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "8. PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "9. PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "10. PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "11. PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "12. PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "13. BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "14. BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "15. BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "16. BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "17. BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "18. BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "19. PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "20. PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "21. PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "22. PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "23. PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "24. PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "25. default.payment.next.month: Default payment (1=yes, 0=no)Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ID'].nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"Number of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "s2Wy6QuzwoPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Duplicate Values\n",
        "# Write your code to make your dataset analysis ready.\n",
        "#renaming dependendent Variable\n",
        "df.rename(columns={'default payment next month' : 'Defaulters'}, inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming some feature name for better understanding of feature\n",
        "df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "\n",
        "df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "\n",
        "df.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ],
      "metadata": {
        "id": "PYc2aVECxjFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "gSUlVaUsxo5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "tlZOxSVBxplq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "hmLJ27bVx2DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bivariate Analysis**"
      ],
      "metadata": {
        "id": "Q1tBF7aCx7lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#analysing categorical variable\n",
        "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']"
      ],
      "metadata": {
        "id": "JhgsjDhPynF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating different df for categorical variable\n",
        "df_cat = df[categorical_features]"
      ],
      "metadata": {
        "id": "ZFHTTbFwypWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking shape of categorical variable\n",
        "df_cat.shape"
      ],
      "metadata": {
        "id": "odm72tPYytFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking into categorical df\n",
        "df_cat.head()"
      ],
      "metadata": {
        "id": "gvDjstaqywhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking value counts of categorical_features\n",
        "for column_name in df_cat.columns:\n",
        "  print(f'count of {column_name} features')\n",
        "  print(f'\\n{df_cat[column_name].value_counts()}\\n')"
      ],
      "metadata": {
        "id": "-2f9IZFMy1G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maping numerical value to categorical for easy understanding\n",
        "df_cat['SEX']=df_cat['SEX'].map({1:'Male',2:'Female'})\n",
        "df_cat['EDUCATION']=df_cat['EDUCATION'].map({1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'other',5:'other',6:'other',0:'other'})\n",
        "df_cat['MARRIAGE']=df_cat['MARRIAGE'].map({1 : 'married', 2 : 'single', 3 : 'others',0 : 'others'})"
      ],
      "metadata": {
        "id": "CA19DVthyplb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat.head()"
      ],
      "metadata": {
        "id": "BrxdGRg0y9gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking value counts of categorical_features\n",
        "for column_name in df_cat.columns:\n",
        "  print(f'count of {column_name} features')\n",
        "  print(f'\\n{df_cat[column_name].value_counts()}\\n')"
      ],
      "metadata": {
        "id": "i93IIcs3zBCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TjNtoFJRzfX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#desribing age variable\n",
        "df['AGE'].describe()"
      ],
      "metadata": {
        "id": "oeb_QoSyzTLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating defaulter variable on df_cat\n",
        "df_cat['Defaulter'] = df['Defaulters']"
      ],
      "metadata": {
        "id": "ZrSw3cMwzmUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat.head()"
      ],
      "metadata": {
        "id": "NeaahcMlzqYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the defaulter ratio of categories\n",
        "for column_name in df_cat.columns[:-1]:\n",
        "  print(df_cat[['Defaulter',column_name]].groupby(column_name).mean().reset_index())"
      ],
      "metadata": {
        "id": "i4LG-JViuXd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def age_cohort(age):\n",
        "  if age in range(21,31) : # starting from 21 because minimum age is 21 in our dataset\n",
        "    return '21-30'\n",
        "  elif age in range(31,41) :\n",
        "    return '31-40'\n",
        "  elif age in range(41,51) :\n",
        "    return '41-50'\n",
        "  elif age in range(51,61) :\n",
        "    return '51-60'\n",
        "  else:\n",
        "    return '60 & above'"
      ],
      "metadata": {
        "id": "SI5m_qNquk_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating age_group column in our data set\n",
        "df['age_group']=df['AGE'].apply(lambda x: age_cohort(x))"
      ],
      "metadata": {
        "id": "Mz3TRQ8WushQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "MUawlnqyvNzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cheacking the count of age_group\n",
        "df['age_group'].value_counts().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "nBaX99IuvO08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop age column because now we have age_group in our dataset\n",
        "df=df.drop('AGE',axis=1)"
      ],
      "metadata": {
        "id": "b4wbCZQivp2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BYPmAAQHvs-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YTG6BtPUzakr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IElvGtIevx3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Payment delay description\n",
        "df[['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']].describe()"
      ],
      "metadata": {
        "id": "VE3W8Anqvz0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Previous Payment Description\n",
        "df[['PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR']].describe()"
      ],
      "metadata": {
        "id": "RjxYHO4Av3F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WfLGYQDxPlL"
      },
      "source": [
        "We can renaming dependendent Variable  and  renaming some feature name for better understanding of feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI4PWxkjxSAA"
      },
      "source": [
        "maping numerical value to categorical for easy understanding\n",
        "\n",
        "Gender (1 = male; 2 = female)\n",
        "\n",
        "Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "\n",
        "Marital status (1 = married; 2 = single; 3 = others)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO7UhgIbxUj_"
      },
      "source": [
        "we can see here min age is 21 and maximum age is 79 in our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTl5tWA2xXik"
      },
      "source": [
        "for Sex feature:\n",
        "  - we can see that  number of females are more than the males\n",
        "  - but the default ratio of male is quite higher than female\n",
        "\n",
        "for Education feature:\n",
        "  - university and High School pesons are more likely to be defaulter in comparison with  the graduate school persons.\n",
        "\n",
        "for Marriage feature:\n",
        "  - people who are not married nor single are more likely to make default of bills in comparison with single and married person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaPNhS49xaGM"
      },
      "source": [
        "In aur dataset we can clearly see that most of the credit card holder are of age between 21 to 40 , so we can say that company's target customer are mostly youngster."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "plt.figure(figsize=(10,8),dpi=80)\n",
        "sns.countplot(x='Defaulters',data=df,palette='Set1')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart displays data using a number of bars, each representing a particular category. The height of each bar is proportional to a specific aggregation (for example the sum of the values in the category it represents). The categories could be something like an age group or a geographical location. It is also possible to color or split each bar into another categorical column in the data, which enables you to see the contribution from different categories to each bar or group of bars in the bar chart.Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we have quite imbalance datasetAnswer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "for column_name in df_cat.columns:\n",
        "  plt.figure(figsize=(10,8),dpi=60)\n",
        "  sns.countplot(x=column_name,data=df_cat,hue=df['Defaulters'],palette=[\"green\",'orange'])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "countplot() method is used to Show the counts of observations in each categorical bin using bars. Parameters : This method is accepting the following parameters that are described below: x, y: This parameter take names of variables in data or vector data, optional, Inputs for plotting long-form data.Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for Sex feature:\n",
        "\n",
        "we can see that number of females are more than the males\n",
        "but the default ratio of male is quite higher than female\n",
        "for Education feature:\n",
        "\n",
        "university and High School pesons are more likely to be defaulter in comparison with the graduate school persons.\n",
        "for Marriage feature:\n",
        "\n",
        "people who are not married nor single are more likely to make default of bills in comparison with single and married personAnswer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10,8),dpi=60)\n",
        "sns.countplot(x=df['age_group'].sort_values(),data=df,hue='Defaulters')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "countplot() method is used to Show the counts of observations in each categorical bin using bars. Parameters : This method is accepting the following parameters that are described below: x, y: This parameter take names of variables in data or vector data, optional, Inputs for plotting long-form data.Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In aur dataset we can clearly see that most of the credit card holder are of age between 21 to 40 , so we can say that company's target customer are mostly youngster.Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "numeric_features= df.select_dtypes(exclude='object')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features"
      ],
      "metadata": {
        "id": "DdaTIbhyyVG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features.info()"
      ],
      "metadata": {
        "id": "4OXzqY2GyYSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "for col in numeric_features[:]:\n",
        "  fig = plt.figure(figsize=(9, 6))\n",
        "  sns.histplot(df[col])\n",
        "  plt.axvline(df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(df[col].median(), color='cyan', linestyle='dashed', linewidth=2)   \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "O2O6OrBaydGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram is a popular graphing tool. It is used to summarize discrete or continuous data that are measured on an interval scale. It is often used to illustrate the major features of the distribution of the data in a convenient form.Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# ploting Regression plot of each columns of dataset v/s Defaulters columns\n",
        "for col in numeric_features[:]:\n",
        "  fig = plt.figure(figsize=(9, 6))\n",
        "  if col == 'Defaulters':\n",
        "    pass\n",
        "  else:\n",
        "    sns.regplot(x=df[col],y=df[\"Defaulters\"],line_kws={\"color\": \"red\"})\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable.Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(x='EDUCATION', y='Defaulters' ,data=df); "
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot or bar chart is a graph that represents the category of data with rectangular bars with lengths and heights that is proportional to the values which they represent. The bar plots can be plotted horizontally or vertically. A bar chart describes the comparisons between the discrete categories. One of the axis of the plot represents the specific categories being compared, while the other axis represents the measured values corresponding to those categories.Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Education (1 = graduate school; 2 = university; 3 = high school; 4,5,6 = others.\n",
        "\n",
        "In aur dataset we can clearly see that most of the credit card holder are of education between graduate school and university , so we can say that company's target customer are mostly youngster.Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Payment delay\n",
        "\n",
        "pay_col = ['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']\n",
        "for col in pay_col:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.countplot(x = col, hue = 'Defaulters', data = df)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(df.corr(),cmap='PiYG',annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "0lmLrY5u0Xci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heat map is a two-dimensional representation of data in which values are represented by colors. A simple heat map provides an immediate visual summary of information. More elaborate heat maps allow the viewer to understand complex data sets.\n",
        "\n",
        "A heat map represents these coefficients to visualize the strength of correlation among variables. It helps find features that are best for Machine Learning model building. The heat map transforms the correlation matrix into color coding\n",
        "\n",
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap.Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above correlation heatmap, we can see Payment delay['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR'] and Bill Statement ['BILL_AMT_SEPT','BILL_AMT_AUG','BILL_AMT_JUL','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR'] are positiveliy highly correlated with a value of 1.Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, hue=\"Defaulters\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation.Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6 . Feature Engineering & Data Pre-processing**"
      ],
      "metadata": {
        "id": "LsELmtqzr_Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Handling Missing Values**"
      ],
      "metadata": {
        "id": "HQfCncpNsOFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ax4T3NNcsbRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No null value found**"
      ],
      "metadata": {
        "id": "bNF9ILzOsfcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        " # Checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"Missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wu-c70_Uskrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What all missing value imputation techniques have you used and why did you use those techniques?**"
      ],
      "metadata": {
        "id": "sB9xbHQgsuRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no missing values to handle in the given dataset.**"
      ],
      "metadata": {
        "id": "CYMW6QHds3Pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Handling Outliers"
      ],
      "metadata": {
        "id": "KtGJO9-Fs-FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking outliers in Defaulters\n",
        "sns.boxplot(df['Defaulters'])"
      ],
      "metadata": {
        "id": "hEauRS_0tGx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "max_thresold = df['Defaulters'].quantile(0.95)\n",
        "max_thresold"
      ],
      "metadata": {
        "id": "DX-XwI6btKqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Defaulters']>max_thresold]"
      ],
      "metadata": {
        "id": "0um2lVh_tQBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_thresold = df['Defaulters'].quantile(0.05)\n",
        "min_thresold"
      ],
      "metadata": {
        "id": "drw58F1WtQPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Defaulters']<min_thresold]"
      ],
      "metadata": {
        "id": "qv3JIwkdtQb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising  code for the numerical columns \n",
        "for col in df.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  sns.stripplot(df[col])"
      ],
      "metadata": {
        "id": "4_FCfTvwtano"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "eCxzb2UKtngF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot is a useful graphical display for describing the behavior of the data in the middle as well as at the ends of the distributions. The box plot uses the median and the lower and upper quartiles (defined as the 25th and 75th percentiles). If the lower quartile is Q1 and the upper quartile is Q3, then the difference (Q3 — Q1) is called the interquartile range or IQ. A box plot is constructed by drawing a box between the upper and lower quartiles with a solid line drawn across the box to locate the median. The following quantities (called fences) are needed for identifying extreme values in the tails of the distribution:\n",
        "\n",
        "The data points which fall below Q1 – 1.5 IQR or above Q3 + 1.5 IQR are outliers. where Q1 and Q3 are the 25th and 75th percentile of the dataset respectively, and IQR represents the inter-quartile range and given by Q3 – Q1. For Other distributions: Use percentile-based approach\n",
        "\n",
        "lower inner fence: Q1–1.5IQ upper inner fence: Q3 + 1.5IQ lower outer fence: Q1–3IQ upper outer fence: Q3 + 3IQ\n",
        "\n",
        "Detecting and Removing outliers using Percentiles"
      ],
      "metadata": {
        "id": "GfCmrZxNttFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "-n8QFp5UtzL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "UjCFvD9pt27U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no categorical columns in the given dataset which I am working on. So, Skipping this part."
      ],
      "metadata": {
        "id": "pd3LyuFrt61S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Textual Data Preprocessing**\n",
        "\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "There are no text columns in the given dataset which I am working on. So, Skipping this part."
      ],
      "metadata": {
        "id": "LVCiRPnit_hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "gUD6fShtuP1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Feature **Selection**"
      ],
      "metadata": {
        "id": "WsBT8umdusP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "#Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "S9qjmkLiusyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Defaulters']]])"
      ],
      "metadata": {
        "id": "QEghtGgKu_8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('BILL_AMT_AUG', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "Udvl2XTovE8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('BILL_AMT_MAY', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "hua33pRevIIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('BILL_AMT_JUL', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "90JAozBqvIRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('ID', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "B-5kfv56vIZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Defaulters']]])"
      ],
      "metadata": {
        "id": "AnafCw1fvWAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply drop those columns whose VIF is greater than 20"
      ],
      "metadata": {
        "id": "luLmrsa_vbqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all feature selection methods have you used and why?"
      ],
      "metadata": {
        "id": "UchZKBzQvgz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Dropping Constant Feature, Dropping columns having multicolinearity and validate through VIF.\n",
        "\n",
        "Steps for Implementing VIF\n",
        "\n",
        "• Calculate the VIF factors.\n",
        "\n",
        "• Inspect the factors for each predictor variable, if the VIF is between 5–10, multicollinearity is likely present and you should consider dropping the variable.\n",
        "\n",
        "In VIF method, we pick each feature and regress it against all of the other features. For each regression, the factor is calculated as :\n",
        "\n",
        "VIF=\\frac{1}{1-R^2}"
      ],
      "metadata": {
        "id": "7QC6r5Jmvk7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting independent Variable\n",
        "X=df.drop(['Defaulters'],axis=1)"
      ],
      "metadata": {
        "id": "1jbcogEivn8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "m6ozAxnuvqVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maping numerical value to categorical for easy understanding\n",
        "X['SEX']=X['SEX'].map({1:'Male',2:'Female'})\n",
        "X['EDUCATION']=X['EDUCATION'].map({1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'other',5:'other',6:'other',0:'other'})\n",
        "X['MARRIAGE']=X['MARRIAGE'].map({1 : 'married', 2 : 'single', 3 : 'others',0 : 'others'})"
      ],
      "metadata": {
        "id": "OsEDl3QAvtdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "bd__d1RJvvmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding for age group\n",
        "X=pd.get_dummies(X)"
      ],
      "metadata": {
        "id": "ds5MWh2Wvvv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "tOe-By7Zv3jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting Dependent variable\n",
        "y=df['Defaulters']"
      ],
      "metadata": {
        "id": "vyJ6wbi6v6_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Handling Imbalanced Dataset**"
      ],
      "metadata": {
        "id": "5M9tUzBfwNan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(df.Defaulters.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "df['Defaulters'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Not Defaulters(%)','Defaulters(%)'],\n",
        "                               colors=['skyblue','red'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "YKNco2MtwVFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think the dataset is imbalanced? Explain Why.\n",
        "mbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n",
        "\n",
        "Imbalance means that the number of data points available for different the classes is different: If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
        "\n",
        "In our case the dataset dependent column data ratio is 78:22. So, during model creating it's obvios that there will be bias and having a great chance of predicting the majority one so frequently. SO the dataset should be balanced before it going for the model creation part.\n",
        "\n",
        "What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)\n",
        "I have used SMOTE (Synthetic Minority Over-sampling technique) for balanced the 78:22 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE."
      ],
      "metadata": {
        "id": "8Ubmg5HdwbSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing SMote to make our dataset balanced\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "X_smote, y_smote = smote.fit_resample(X,y)\n",
        "\n",
        "print('Original dataset shape', len(df))\n",
        "print('Resampled dataset shape', len(y_smote))"
      ],
      "metadata": {
        "id": "c-3l0y5swe7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now checking the count of dependent variaable after balancing\n",
        "(unique, counts) = np.unique(y_smote, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ],
      "metadata": {
        "id": "sxV5IwX-wigL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Data Splitting**"
      ],
      "metadata": {
        "id": "umTgNjvswm9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting data set into train and test dataset\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_smote,y_smote,test_size=0.25,random_state=101)"
      ],
      "metadata": {
        "id": "V7I5CNIqws-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking into independent variable of train dataset\n",
        "X_train"
      ],
      "metadata": {
        "id": "cmQZGVBBwv-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "5UBTT1vIwzQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What data splitting ratio have you used and why?\n",
        "In this case the training dataset is small, that's why I have taken 70:30 ratio.\n",
        "\n",
        "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
        "\n",
        "You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio."
      ],
      "metadata": {
        "id": "GxuAvDSOw35x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. ML Model Implementation"
      ],
      "metadata": {
        "id": "aaOxmZlbw7aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Function"
      ],
      "metadata": {
        "id": "BLLIEVw1w-yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_accuracy=[]\n",
        "model_precision=[]\n",
        "model_recall=[]\n",
        "model_f1_score=[]\n",
        "model_roc_auc_score=[]\n"
      ],
      "metadata": {
        "id": "IYxPzug8xCyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Function to run diferent models\n",
        "def run_and_evaluate_model(model,X_train,X_test,y_train,y_test, best_parameter=True, best_score=True):\n",
        "  '''\n",
        "  train the model and gives mse,rmse,r2,adj r2 score of the model\n",
        "  can be used for any model where y is not transformed \n",
        "  '''\n",
        "\n",
        "  start=time.time()\n",
        "  #training the model\n",
        "  model.fit(X_train,y_train)\n",
        "  stop = time.time()\n",
        "\n",
        "  time_min=round((stop - start)/60,4)\n",
        "  print(f\"Training time: {time_min}min\",'\\n')\n",
        "  \n",
        "\n",
        "\n",
        "  #predicting the values of y from x via model\n",
        "  y_pred_test = model.predict(X_test)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "\n",
        "  \n",
        "  # Get the confusion matrix for both train and test\n",
        "  from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "  def score (model,X,actual,predicted,append=True):\n",
        "    accuracy = accuracy_score(actual,predicted)\n",
        "    precision = precision_score(actual,predicted)\n",
        "    recall = recall_score(actual,predicted)\n",
        "    f1 = f1_score(actual,predicted)\n",
        "    roc= roc_auc_score(actual,predicted)\n",
        "    confusion_mat=confusion_matrix(actual,predicted)\n",
        "    print(\"The accuracy is \", accuracy)\n",
        "    print(\"The precision is \", precision)\n",
        "    print(\"The recall is \", recall)\n",
        "    print(\"The f1 is \", f1)\n",
        "    print('the roc_auc_score  is ',roc)\n",
        "    print('\\nconfusion_matrix \\n ',confusion_mat)\n",
        "    \n",
        "\n",
        "    if append==True:\n",
        "      model_accuracy.append(accuracy)\n",
        "      model_precision.append(precision)\n",
        "      model_recall.append(recall)\n",
        "      model_f1_score.append(f1)\n",
        "      model_roc_auc_score.append(roc)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  print('score matrix for train')\n",
        "  print('*'*80)\n",
        "  score(model=model,X=X_train,actual=y_train,predicted=y_pred_train,append=False)\n",
        "  print('\\nClassification Report\\n')\n",
        "  print(classification_report(y_train, y_pred_train))\n",
        "  print('\\n')\n",
        "  print('score matrix for test')\n",
        "  print('*'*80)\n",
        "  score(model=model,X=X_test,actual=y_test,predicted=y_pred_test)\n",
        "  print('\\nClassification Report\\n')\n",
        "  print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "HPahVKBzxK1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating function to get feature importance"
      ],
      "metadata": {
        "id": "FdAffWcDxTcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function to get features importance of all the tree based model\n",
        "def get_features_importance(optimal_model,X_train):\n",
        "  imp_feat=pd.DataFrame(index=X.columns,data=optimal_model.feature_importances_,columns=['importance'])\n",
        "  imp_feat=imp_feat[imp_feat['importance']>0]\n",
        "  imp_feat=imp_feat.sort_values('importance')\n",
        "  plt.figure(figsize=(15,5))\n",
        "  print(f'==========================Features Importance============================\\n\\n {optimal_model}\\\n",
        "  \\n=========================================================================\\n') \n",
        "  sns.barplot(data=imp_feat,x=imp_feat.index,y='importance')\n",
        "  plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "e26htIipxZe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 1 -- Implementing Logistic Regression"
      ],
      "metadata": {
        "id": "YQ5nZW-lxr11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import Loggistic Regression\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "fNfsOTLXxxrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "log_model = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "# Fit the Algorithm\n",
        "log_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ZMTgOBL5xyvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "log_model.coef_"
      ],
      "metadata": {
        "id": "LxuGYZtSx5N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "log_model.intercept_"
      ],
      "metadata": {
        "id": "Sc-b22f7x-DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "train_preds = log_model.predict_proba(X_train)\n",
        "test_preds = log_model.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "HWiUQ8gwx-Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds"
      ],
      "metadata": {
        "id": "EjCorwDAyDPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds"
      ],
      "metadata": {
        "id": "8Z_UY_pEyDSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "rllkaCZ6yKge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#running and evaluating log_model using function ccreated\n",
        "run_and_evaluate_model(log_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "9G3KDvfcyODZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Logistic regression algorithm to create the model. As I got not so good result. \n",
        "\n",
        "For training dataset, i found precision of 55% and recall of 79% and f1-score of 64% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 62% and recall of 35% and f1-score of 44%. Accuracy is 56% and average percision, recall & f1_score are 62%, 34% and 44% respectively with a roc auc score of 56%.\n",
        "\n",
        "For testing dataset, i found precision of 62% and recall of 59% and f1-score of 60% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 61% and recall of 64% and f1-score of 62%. Accuracy is 61% and average percision, recall & f1_score are 60%, 63% and 62% respectively with a roc auc score of 61%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique.\n",
        "\n"
      ],
      "metadata": {
        "id": "YlJAInY8aeVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "o87ZyHLc3mgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GridSearch CV"
      ],
      "metadata": {
        "id": "6eOINCAk3uZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating param dict for hyperparameter tuning\n",
        "param_dict= {'C': [0.001,0.01,0.1,1,10,100],'penalty': ['l1', 'l2'],'max_iter':[1000]} "
      ],
      "metadata": {
        "id": "6JzN2q543xvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Grid model to perform grid search\n",
        "\n",
        "grid_log_model = GridSearchCV(log_model, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall') "
      ],
      "metadata": {
        "id": "mJ3AdVsZ3xyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running and evaluating grid_log_model using function ccreated\n",
        "run_and_evaluate_model(grid_log_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "ZE8GLPNh3x1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "hDoTIWGw4FQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "93XcHr6h4JkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "pS64WRMT4NtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For training dataset, i found precision of 55% and recall of 79% and f1-score of 64% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 62% and recall of 35% and f1-score of 44%. Accuracy is 56% and average percision, recall & f1_score are 6%, 34% and 44% respectively with a roc auc score of 56%.\n",
        "\n",
        "NO improvment seen in precsion, f1 score,accuracy and roc auc score\n",
        "\n",
        "For testing dataset, i found precision of 55% and recall of 78% and f1-score of 65% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 62% and recall of 36% and f1-score of 45%. Accuracy is 56% and average percision, recall & f1_score are 62%, 35% and 45% respectively with a roc auc score of 56%.\n",
        "\n",
        "NO improvemnt seen in precsion, f1 score and roc auc"
      ],
      "metadata": {
        "id": "Z7DYYxxB4Qpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features Importance**"
      ],
      "metadata": {
        "id": "c7Spldvi4UP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the coeficient of best estimator\n",
        "grid_log_model.best_estimator_.coef_"
      ],
      "metadata": {
        "id": "6jgILKsu4Xai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_log_model=grid_log_model.best_estimator_.coef_"
      ],
      "metadata": {
        "id": "GZLTg12H4adQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ccreating dataframe for feature imp\n",
        "feature_importance = pd.DataFrame({'Features':X.columns, 'Importance':np.abs(optimal_log_model).ravel() })"
      ],
      "metadata": {
        "id": "w9TjDvvZ4fen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sorting the feat impt df by importance\n",
        "imp_feat = feature_importance.sort_values(by = 'Importance', ascending=False)[:10]"
      ],
      "metadata": {
        "id": "GThQ78Pz41ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_feat"
      ],
      "metadata": {
        "id": "AjZ1Ybeb46ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the feature importance\n",
        "imp_feat=imp_feat[imp_feat['Importance']>0]\n",
        "imp_feat=imp_feat.sort_values('Importance')\n",
        "plt.figure(figsize=(15,5))\n",
        "print(f'==========================Features Importance============================\\n\\n {optimal_log_model}\\\n",
        "\\n=========================================================================\\n') \n",
        "sns.barplot(data=imp_feat,x=imp_feat.Features,y='Importance')\n",
        "plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "1Y3melJ45MN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 2 - Implementing Decision Tree**"
      ],
      "metadata": {
        "id": "CU_0HcWT5hJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#impoting decisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# creating DecisionTree model\n",
        "DecisionTree = DecisionTreeClassifier(max_depth=10,max_leaf_nodes=45,criterion='entropy')\n",
        "\n",
        "# training and evaluating the DecisionTree\n",
        "run_and_evaluate_model(DecisionTree,X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "LlE7ll_o5mXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DecisionTree.feature_importances_"
      ],
      "metadata": {
        "id": "RkiG4kFJ5rl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Decision Tree algorithm to create the model. As I got better result to Logistic regression result.\n",
        "\n",
        "For training dataset, i found precision of 78% and recall of 91% and f1-score of 84% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 89% and recall of 74% and f1-score of 81%. Accuracy is 82% and average percision, recall & f1_score are 88%, 74% and 80% respectively with a roc auc score of 82%.\n",
        "\n",
        "For testing dataset, i found precision of 77% and recall of 92% and f1-score of 84% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 90% and recall of 73% and f1-score of 81%. Accuracy is 82% and average percision, recall & f1_score are 89%, 73% and 80% respectively with a roc auc score of 82%."
      ],
      "metadata": {
        "id": "BrUUimXk5v9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_features_importance(DecisionTree,X_train)"
      ],
      "metadata": {
        "id": "PDVQcirf7z-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 3 -- Implementing Random Forest**"
      ],
      "metadata": {
        "id": "19kYGLbG738t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc_model=RandomForestClassifier()"
      ],
      "metadata": {
        "id": "KtynKQ6E78IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the Random forest\n",
        "run_and_evaluate_model(rfc_model,X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "AxAqXYUg78TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Random Forest algorithm to create the model. As I got so good result.\n",
        "\n",
        "For training dataset, i found precision of 1% and recall of 1% and f1-score of 1% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 1% and recall of 1% and f1-score of 1%. Accuracy is 99% and average percision, recall & f1_score are 99%, 99% and 99% respectively with a roc auc score of 99%.\n",
        "\n",
        "For testing dataset, i found precision of 85% and recall of 92% and f1-score of 88% for our data. BUt, I am also interested to see the result for Defaulters as I got precision of 91% and recall of 83% and f1-score of 87%. Accuracy is 87% and average percision, recall & f1_score are 91%, 83% and 87% respectively with a roc auc score of 87%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "YPs7QErY8EuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "Axlox0MJ8Ihm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for rfc_model by gridsearchcv\n",
        "grid_values = {'n_estimators': [150,200,250],'max_depth': [3,5,9],'criterion': ['entropy']}\n",
        "grid_rfc_model = GridSearchCV(estimator=rfc_model,param_grid = grid_values, scoring='balanced_accuracy',cv=5,verbose=5,n_jobs=-1)"
      ],
      "metadata": {
        "id": "qGzf5Af28M1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the Random forest with hyperparameter tuing\n",
        "run_and_evaluate_model(grid_rfc_model,X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "ZE-s3DCn8M6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the best estimator for random forest\n",
        "grid_rfc_model.best_estimator_"
      ],
      "metadata": {
        "id": "2DRtoJTQ8hZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing best estimator in varibale\n",
        "optimal__rfc_model=grid_rfc_model.best_estimator_"
      ],
      "metadata": {
        "id": "zjs5UbFn8kBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the feature importance of variable for random forest\n",
        "get_features_importance(optimal__rfc_model,X_train)"
      ],
      "metadata": {
        "id": "maUFXVJr8nqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 4 -- Implementing Support Vector Machine**"
      ],
      "metadata": {
        "id": "SkQXh39i8zo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing SVM\n",
        "from sklearn.svm import SVC\n",
        "svc_model=SVC()"
      ],
      "metadata": {
        "id": "BlFTK5LX9jzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the SVC model \n",
        "run_and_evaluate_model(svc_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "ZAvreoNg9mOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 5 -- Implementing XGboost**"
      ],
      "metadata": {
        "id": "S7DhJrmN9qGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "Z1DH_JiJ9uGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating XGBRegressor model\n",
        "xgboost_model=XGBClassifier()"
      ],
      "metadata": {
        "id": "U_nZMOVR9wY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the Xgboost model\n",
        "run_and_evaluate_model(xgboost_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "4A_cEpFV9y2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning for Xgboost"
      ],
      "metadata": {
        "id": "ocyqtXkK93M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "params={'n_estimators': [50,100,150],'max_depth': [3,5,9]}\n",
        "grid_xgboost_model=GridSearchCV(estimator=xgboost_model,param_grid=params,cv=5,scoring='recall',verbose=5,n_jobs=-1)"
      ],
      "metadata": {
        "id": "z8UGX57m99vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the xgb_grid\n",
        "run_and_evaluate_model(grid_xgboost_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "F4R9hpM9-ASn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visaulizing feature importance\n",
        "get_features_importance(xgboost_model,X_train)"
      ],
      "metadata": {
        "id": "gpwnVos1-DQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 5 -- Implementing CatBoost**"
      ],
      "metadata": {
        "id": "uJB_uE9C_T1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "Xvd3JlL8_vHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier"
      ],
      "metadata": {
        "id": "W7TqVsFyAX1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_model=CatBoostClassifier(verbose=0)"
      ],
      "metadata": {
        "id": "11BdDla-AbBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_model=CatBoostClassifier()"
      ],
      "metadata": {
        "id": "_fSaU8PtAg8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the cb model\n",
        "run_and_evaluate_model(cb_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "mSIlbLYmBsO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tuning for Catboost**"
      ],
      "metadata": {
        "id": "PykC3CrMB1rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "params={'n_estimators': [50,100,150],'max_depth': [3,5,9]}\n",
        "grid_cb_model=GridSearchCV(estimator=cb_model,param_grid=params,cv=3,scoring='recall',verbose=0,n_jobs=-1)"
      ],
      "metadata": {
        "id": "eHQ89jxsB5_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the catboost model with hyperparameter tuing\n",
        "run_and_evaluate_model(grid_cb_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "8AQsmcGVB8pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing feature importance\n",
        "get_features_importance(cb_model,X_train)"
      ],
      "metadata": {
        "id": "6AxumNj6CACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model's Score matrix"
      ],
      "metadata": {
        "id": "fKZF8-5cCC5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dictionary to store all the metrices \n",
        "dict={'accuracy':model_accuracy,'precision':model_precision,'recall':model_recall,'f1':model_f1_score,'roc_auc':model_roc_auc_score}"
      ],
      "metadata": {
        "id": "PEMTEgOdCHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of all models\n",
        "model_name=['Logestic Regrestion','grid_log_regg','Desision Tree','Random forest','grid random forest','SVM','XGboost','Grid Xgboost','CATBoost','Grid Catboost']"
      ],
      "metadata": {
        "id": "4trsgxghCIG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting dictionary to dataframe\n",
        "matrix_df=pd.DataFrame.from_dict(dict,orient=\"index\",columns=model_name)"
      ],
      "metadata": {
        "id": "pUyMtXxfCITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking the transpose of the dataframe to make it more visual appealing\n",
        "matrix_df=matrix_df.transpose().reset_index().rename(columns={'index':'Models'})"
      ],
      "metadata": {
        "id": "C7OYO70uCUXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_df"
      ],
      "metadata": {
        "id": "W537VvL_CW6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "r4mjdNaGDCad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We found that using Random forest and Grid Xgboost and CATBoost are better model in all ."
      ],
      "metadata": {
        "id": "EE92OGFuDGXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hurrah! You have successfully completed your Machine Learning Capstone Project !!!"
      ],
      "metadata": {
        "id": "npQI6Cu-DK0k"
      }
    }
  ]
}